MULTILABEL EXPERIMENT REPORT
Generated Mon Jul 10 20:36:15 2023

Args:  -b 26 --learning_rate 1.5 --passes 30 --csoaa 35 --kill_cache --cache_file a.cache

mix_test_tagset

EXPERIMENT WITH 164 TEST CHANGESETS
F1 SCORE : 0.547 weighted, 0.431 micro-avg'd, 0.322 macro-avg'd
PRECISION: 0.508 weighted, 0.306 micro-avg'd, 0.242 macro-avg'd
RECALL   : 0.727 weighted, 0.727 micro-avg'd, 0.856 macro-avg'd

 -----------------CLASSIFICATION REPORT-----------------
                          precision    recall  f1-score   support

              SQLAlchemy       0.13      1.00      0.24         2
                  Scrapy       0.13      1.00      0.24         2
                  Theano       0.13      1.00      0.24         2
                 astropy       0.13      1.00      0.24         2
          beautifulsoup4       0.13      1.00      0.24         2
               biopython       0.13      1.00      0.24         2
                   bokeh       0.13      1.00      0.24         2
                   boto3       0.67      0.60      0.63        30
                   cmake       0.83      0.61      0.70        31
                    dask       0.13      1.00      0.24         2
                    deap       0.13      1.00      0.24         2
                  jinja2       0.65      0.81      0.72        21
              matplotlib       0.58      0.60      0.59        30
                networkx       0.00      0.00      0.00         0
                 nilearn       0.13      1.00      0.24         2
  nvidia-cuda-nvrtc-cu11       0.52      0.57      0.55        21
nvidia-cuda-runtime-cu11       0.13      1.00      0.24         2
           opencv-python       0.13      1.00      0.24         2
                  pandas       0.60      0.92      0.73        26
                  pillow       0.28      1.00      0.43         5
                  plotly       0.13      1.00      0.24         2
                 pycaret       0.12      1.00      0.21         2
                 pyspark       0.13      1.00      0.24         2
                   redis       0.13      1.00      0.24         2
                requests       0.13      1.00      0.24         2
            scikit-image       0.00      0.00      0.00         0
            scikit-learn       0.13      1.00      0.24         2
                   scipy       0.57      0.91      0.70        22
                   scoop       0.13      1.00      0.24         2
              simplejson       0.13      1.00      0.24         2
                     six       0.15      1.00      0.27         2
             statsmodels       0.13      1.00      0.24         2
                  triton       0.20      0.20      0.20        20
           triton==2.0.0       0.11      1.00      0.19         2
                   wheel       0.54      0.74      0.62        19

               micro avg       0.31      0.73      0.43       271
               macro avg       0.24      0.86      0.32       271
            weighted avg       0.51      0.73      0.55       271
             samples avg       0.82      0.80      0.75       271

