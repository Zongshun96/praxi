MULTILABEL EXPERIMENT REPORT
Generated Mon Jul 10 20:37:48 2023

Args:  -b 26 --learning_rate 1.5 --passes 30 --csoaa 35 --kill_cache --cache_file a.cache

large_mix_test_tagset

EXPERIMENT WITH 1175 TEST CHANGESETS
F1 SCORE : 0.744 weighted, 0.753 micro-avg'd, 0.732 macro-avg'd
PRECISION: 0.794 weighted, 0.807 micro-avg'd, 0.780 macro-avg'd
RECALL   : 0.705 weighted, 0.705 micro-avg'd, 0.694 macro-avg'd

 -----------------CLASSIFICATION REPORT-----------------
                          precision    recall  f1-score   support

              SQLAlchemy       0.85      0.86      0.85        70
                  Scrapy       0.84      0.78      0.81        76
                  Theano       0.82      0.73      0.77        70
                 astropy       0.83      0.77      0.80        70
          beautifulsoup4       0.85      0.81      0.83        70
               biopython       0.82      0.70      0.75        70
                   bokeh       0.78      0.57      0.66        70
                   boto3       0.80      0.72      0.75        60
                   cmake       0.91      0.69      0.79        61
                    dask       0.83      0.76      0.79        70
                    deap       0.83      0.77      0.80        70
                  jinja2       0.80      0.69      0.74        62
              matplotlib       0.79      0.68      0.73        60
                networkx       0.82      0.72      0.77        68
                 nilearn       0.80      0.64      0.71        70
  nvidia-cuda-nvrtc-cu11       0.82      0.68      0.75        60
nvidia-cuda-runtime-cu11       0.83      0.74      0.78        66
           opencv-python       0.83      0.74      0.78        70
                  pandas       0.78      0.79      0.78        62
                  pillow       0.80      0.72      0.76        65
                  plotly       0.75      0.53      0.62        62
                 pycaret       0.68      0.44      0.53        64
                 pyspark       0.83      0.77      0.80        70
                   redis       0.82      0.73      0.77        70
                requests       0.83      0.76      0.79        66
            scikit-image       0.84      0.85      0.85        68
            scikit-learn       0.81      0.70      0.75        66
                   scipy       0.81      0.86      0.83        63
                   scoop       0.83      0.76      0.79        70
              simplejson       0.85      0.76      0.80        70
                     six       0.86      0.80      0.83        70
             statsmodels       0.83      0.75      0.79        64
                  triton       0.19      0.22      0.21        18
           triton==2.0.0       0.17      0.05      0.07        66
                   wheel       0.78      0.74      0.76        54

               micro avg       0.81      0.71      0.75      2281
               macro avg       0.78      0.69      0.73      2281
            weighted avg       0.79      0.71      0.74      2281
             samples avg       0.87      0.71      0.76      2281

